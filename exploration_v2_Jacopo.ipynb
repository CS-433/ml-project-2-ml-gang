{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Notebook v2 - Jacopo\n",
    "\n",
    "**Version**: v2\n",
    "\n",
    "## I just Realized I've been using the 10% datasets the whole time...\n",
    "\n",
    "Use glove.twitter.27B as embeddings and/or better preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(df, vocab, embeddings, mode='avg'):\n",
    "    X = np.zeros((df.shape[0], embeddings.shape[1]))\n",
    "    for i, tweet in enumerate(df['tweet']):\n",
    "        words = tweet.split()\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                X[i] += embeddings[vocab[word]]\n",
    "        if mode == 'avg':\n",
    "            X[i] /= len(words)\n",
    "        elif mode == 'sum':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(mode))\n",
    "    return X\n",
    "def load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt'):\n",
    "    # Load data, txt as csv\n",
    "    #data_path = 'data/twitter-datasets/'\n",
    "    df_train_pos = pd.read_csv(path_pos, sep = '\\t', names = ['tweet'])\n",
    "    df_train_pos['label'] = 1\n",
    "    df_train_neg = pd.read_csv(path_neg, sep = '\\t', names = ['tweet'])\n",
    "    df_train_neg['label'] = 0\n",
    "    df_train = pd.concat([df_train_pos, df_train_neg])\n",
    "    print('Train set: ', df_train.shape)\n",
    "    print('Train set positives: ', df_train_pos.shape)\n",
    "    print('Train set negatives: ', df_train_neg.shape)\n",
    "    return df_train   \n",
    "def load_test_data():\n",
    "    # Load test data: id, tweet for each row\n",
    "    data_path = 'data/twitter-datasets/'\n",
    "    df_test = pd.read_csv(data_path + 'test_data.txt', header=None, names=['line'], sep='\\t')\n",
    "    # Extract id and tweet, limit split by 1 so we don't split the tweet (this is v0, at least we keep it intact)\n",
    "    df_test['id'] = df_test['line'].apply(lambda x: x.split(',',1)[0]) \n",
    "    df_test['tweet'] = df_test['line'].apply(lambda x: x.split(',',1)[1])\n",
    "    df_test = df_test.drop('line', axis=1)\n",
    "    return df_test\n",
    "def predict_test_data(X_test, classifier, filename='submission.csv'):\n",
    "    # Predict test data and save to csv\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    df_test['Prediction'] = y_pred\n",
    "    df_test.rename(columns={'id': 'Id'}, inplace=True)\n",
    "    df_test['Prediction'] = df_test['Prediction'].apply(lambda x: -1 if x == 0 else x)\n",
    "    df_test.to_csv(filename, columns=['Id', 'Prediction'], index=False)\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load data, txt as csv\n",
    "data_path = 'data/twitter-datasets/'\n",
    "df_train_pos = pd.read_csv(data_path + 'train_pos_full.txt', sep = '\\t', names = ['tweet'])\n",
    "df_train_pos['label'] = 1\n",
    "df_train_neg = pd.read_csv(data_path + 'train_neg_full.txt', sep = '\\t', names = ['tweet'], on_bad_lines='skip')\n",
    "df_train_neg['label'] = 0\n",
    "df_train = pd.concat([df_train_pos, df_train_neg])\n",
    "print('Train set: ', df_train.shape)\n",
    "print('Train set positives: ', df_train_pos.shape)\n",
    "print('Train set negatives: ', df_train_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                            word\n",
      "line                                                            \n",
      "<user> 0.62415 0.62476 -0.082335 0.20101 -0.137...        <user>\n",
      ". 0.69586 -1.1469 -0.41797 -0.022311 -0.023801 ...             .\n",
      ": 1.1242 0.054519 -0.037362 0.10046 0.11923 -0....             :\n",
      "rt 0.74056 0.9155 -0.16352 0.35843 0.05266 0.14...            rt\n",
      ", 0.84705 -1.0349 -0.050419 0.27164 -0.58659 0....             ,\n",
      "<repeat> 0.67867 -0.74651 -0.31831 -0.093681 0....      <repeat>\n",
      "<hashtag> 0.18227 -0.29194 -1.3632 -1.201 0.084...     <hashtag>\n",
      "<number> 1.3956 0.2892 0.48572 -1.1412 0.21461 ...      <number>\n",
      "<url> 0.80384 -1.0366 -0.53877 -1.0806 0.84718 ...         <url>\n",
      "! 0.4049 -0.87651 -0.23362 -0.34844 -0.097002 0...             !\n",
      "i -0.26079 0.59108 0.61622 -0.70368 -0.85159 -0...             i\n",
      "a 0.21294 0.31035 0.17694 0.87498 0.067926 0.59...             a\n",
      " 1.0822 -0.59378 -0.19992 0.66626 0.18051 0.014...              \n",
      "stressfree -1.399 0.8163 -1.2128 -1.0678 -0.364...    stressfree\n",
      "susceptibles -0.080434 -0.76225 -2.5135 0.96496...  susceptibles\n",
      "tanımaz 0.016892 -0.86062 0.14096 -0.55439 -0.3...       tanımaz\n",
      "telepatía -0.31643 0.83829 0.045491 0.74578 0.9...     telepatía\n",
      "teuer 0.41671 -1.4797 -1.159 -2.2366 0.17963 0....         teuer\n",
      "thebeatles -1.2485 -0.21687 0.13259 -0.59983 -1...    thebeatles\n",
      "trabalhou 0.23493 -0.24535 1.0852 -1.111 -0.005...     trabalhou\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/60pqr7fj1bd13wyw3nwdcbb80000gn/T/ipykernel_49354/2647447453.py:3: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  vocab_embeddings['word'] = vocab_embeddings.index.str.split(' ', 1).str[0]\n"
     ]
    }
   ],
   "source": [
    "# csv with word next to its embedding of d = 200\n",
    "vocab_embeddings = pd.read_csv('data/glove/glove.twitter.27B.25d.txt', sep='\\r', index_col=0, names=['line'], nrows=10000)\n",
    "vocab_embeddings['word'] = vocab_embeddings.index.str.split(' ', 1).str[0]\n",
    "vocab_embeddings.head(20)\n",
    "print(vocab_embeddings.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = build_feature_matrix(df_train, vocab, embeddings, mode='avg')\n",
    "y_train_full = df_train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train_full shape: ', X_train_full.shape)\n",
    "print('y_train_full shape: ', y_train_full.shape)\n",
    "print('Embeddings shape: ', embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# let's try a shallower structure \n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=250, \n",
    "    max_depth=5,\n",
    "    n_jobs=-1,\n",
    "    min_samples_split=15,\n",
    "    verbose=2\n",
    ")\n",
    "scores = cross_val_score(clf, X_train_full, y_train_full, cv=5)\n",
    "print('Cross validation scores: ', scores)\n",
    "print('Mean cross validation score: ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data: id, tweet for each row\n",
    "df_test = load_test_data()\n",
    "X_test = build_feature_matrix(df_test, vocab, embeddings, mode='avg')\n",
    "\n",
    "# pred\n",
    "predict_test_data(X_test, clf, filename='data/out/submission-v2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c927aa12d5ce5f7072c92979fe584a1fce73005a0de16af9e5cbcd0d6c1397"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
